# ğŸš€ **Python Parallel Text Handling Processor**

A high-performance, scalable, multi-threaded **Text Processing, Rule-Based Scoring, Search, Storage & Analytics System** built entirely in Python.

This project is designed for **fast batch processing**, **parallel text chunking**, **rule-based compliance scoring**, **searchable history**, **email summaries**, and an **interactive Streamlit dashboard** â€” all optimized for large-scale text workflows.

---

## ğŸ“Œ **Features at a Glance**

* âš¡ **Parallel Text Chunking** (Multi-threaded)
* ğŸ“š **Rule-Based Checker & Scoring Engine**
* ğŸ§  **Storage Improver** â€” Auto-suggests new rules
* ğŸ—ƒï¸ **SQLite Storage with Hash-level Deduplication**
* ğŸ” **Powerful Search Engine (regex & keyword)**
* ğŸ“¤ **CSV Exporter & Email Summaries**
* ğŸ“Š **Analytics Dashboard (Streamlit UI)**
* ğŸ“ **PDF Reporting**
* ğŸ“ **Full Folder Pipeline Support**
* ğŸ›  **Extensible architecture â€” Plug-and-Play Modules**

---

# ğŸ“‚ **Project Folder Structure**

```
project/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ checker/
â”‚   â”‚   â”œâ”€â”€ checker.py
â”‚   â”‚   â””â”€â”€ rules.py
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ storage.py
â”‚   â”‚   â””â”€â”€ storage_improver.py
â”‚   â”œâ”€â”€ search_export/
â”‚   â”‚   â”œâ”€â”€ search_save.py
â”‚   â”‚   â””â”€â”€ emailer.py
â”‚   â”œâ”€â”€ text_processing/
â”‚   â”‚   â”œâ”€â”€ text_breaker.py
â”‚   â”‚   â”œâ”€â”€ text_loader.py
â”‚   â”‚   â””â”€â”€ parallel_break_loader.py
â”‚   â”œâ”€â”€ utils.py
â”‚
â”‚â”€â”€ data/
â”‚â”€â”€ output/
â”‚â”€â”€ improver_output/
â”‚â”€â”€ streamlit_app.py
â”‚â”€â”€ run.py
â”‚â”€â”€ .env
â”‚â”€â”€ .gitignore
â”‚â”€â”€ README.md
```

---

# ğŸ§  **Project Overview**

This system processes large volumes of text using **parallel execution**, breaks them into **word-based chunks**, runs them through a **rule-based scoring engine**, stores each chunk with metadata, and exposes:

ğŸ‘‰ A complete **end-to-end automated pipeline** via `run.py`
ğŸ‘‰ A full **interactive GUI dashboard** via `streamlit_app.py`

The system is made for **linguistic analysis**, **text compliance auditing**, **sentiment scoring**, and **large-scale text mining**.

---

# ğŸ— **System Architecture**

### **1ï¸âƒ£ Text Ingestion**

* Loads `.txt` files individually or as a folder batch
* Cleans text (whitespace normalization)

### **2ï¸âƒ£ Parallel Chunking**

Implemented in `parallel_break_loader.py`

* Breaks text by **word groups**
* Assigns UIDs
* Computes SHA-256 hash for deduplication

### **3ï¸âƒ£ Rule-Based Scoring Engine**

`checker.py` + `rules.py`

* Loads `rules.json`
* Applies regex/keyword rules
* Computes score
* Stores rule hits ("details")

### **4ï¸âƒ£ Storage Layer**

`storage.py`

* SQLite database
* Stores chunks, scores, timestamps
* Prevents duplicates based on text hash
* Provides fast querying

### **5ï¸âƒ£ Storage Improver**

`storage_improver.py`

* Auto-suggests new rules
* Finds frequent repeated patterns
* Helps improve rule quality

### **6ï¸âƒ£ Search & Export**

`search_save.py`

* Keyword/regex search
* Save results to CSV

### **7ï¸âƒ£ Email Summary (Optional)**

`emailer.py`

* Builds automated email summaries
* Sends alerts if scoring crosses threshold

### **8ï¸âƒ£ Streamlit UI Dashboard**

`streamlit_app.py`

* File upload & management
* Pipeline execution
* Record browser
* Analytics
* Rule manager
* PDF reporting
* Storage improver UI

---

# ğŸš€ **End-to-End Pipeline Workflow**

When you run:

```
python run.py
```

The system automatically performs:

1ï¸âƒ£ Load rules from `rules.json` <br>
2ï¸âƒ£ Load & clean text files <br>
3ï¸âƒ£ Chunk into groups <br>
4ï¸âƒ£ Deduplicate using SHA-256 hashes <br>
5ï¸âƒ£ Run parallel rule-based scoring <br>
6ï¸âƒ£ Store results in SQLite DB <br>
7ï¸âƒ£ Run storage improver <br>
8ï¸âƒ£ Perform sample search <br>
9ï¸âƒ£ Export results to CSV <br>
ğŸ”Ÿ Generate email summary <br>

---

# ğŸ–¥ï¸ **Using the Streamlit Dashboard**

Run:

```
streamlit run streamlit_app.py
```

### The dashboard provides:

* File Upload Manager
* Pipeline Runner
* Overview Metrics
* Search & Filtering
* Chunk Browser
* Score Analytics
* Wordcloud
* Rule Hits Chart
* Storage Improver
* Rules.json Editor
* PDF Report Builder

---

# ğŸ§© **Key Modules Explained**

### 1. **parallel_break_loader.py**

Handles full pipeline:

* Chunking
* Deduplication
* Scoring
* Saving
* Parallel execution

### 2. **text_breaker.py**

* Cleans text
* Splits into fixed-size word groups

### 3. **checker.py**

* Applies rules
* Scores text
* Stores results

### 4. **storage.py**

* SQLite backend
* Query builder
* Hash existence check

### 5. **search_save.py**

* Regex / keyword search
* CSV export

### 6. **emailer.py**

* Email summary
* HTML email generator

### 7. **storage_improver.py**

* Auto-rule suggestions based on DB frequency

---

# âš™ï¸ **Installation & Setup**

### **1. Create Virtual Environment**

```
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### **2. Install Dependencies**

```
pip install -r requirements.txt
```

### **3. Environment Variables**

Create `.env` file:

```
DB_PATH=checks.db
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
EMAIL_ADDRESS=youremail@gmail.com
EMAIL_PASSWORD=yourapppassword
```

### **4. Folder Setup**

```
mkdir data/support_text_files
mkdir output
mkdir improver_output
```

---

# ğŸ§ª **Running the Pipeline**

```
python run.py
```

# ğŸ–¥ï¸ **Running the Streamlit App**

```
streamlit run streamlit_app.py
```

---

# ğŸ“Š **Sample Outputs**

* Processed chunks
* Scores
* Applied rule IDs
* CSV exports
* PDF Reports
* Suggested rules
* Email summary

---

# ğŸ¤– **Storage Improver (Rule Auto-Generator)**

AI-like rule analyzer that:

* scans high-frequency words/phrases
* detects missing rules
* suggests new rule patterns

Output â†’ JSON suggestions saved in:

```
improver_output/suggestions.json
```

---

# ğŸ“§ **Email Summary**

Automatically compiles:

* recent scores
* high severity alerts
* rule hit summary

You can enable/disable email sending in `run.py`.

---

# ğŸ›¡ **Deduplication Logic**

Before scoring, each chunk is hashed:

```
sha256(text)
```

If hash already exists in DB â†’ **skipped**.
This saves compute and prevents duplicates.

---

# ğŸ§± **Tech Stack**

| Component     | Technology               |
| ------------- | ------------------------ |
| Language      | Python                   |
| DB            | SQLite                   |
| UI            | Streamlit                |
| Parallelism   | ThreadPoolExecutor       |
| Email         | SMTP                     |
| Reports       | ReportLab                |
| Visualization | Plotly, WordCloud        |
| Logging       | Python Logging Framework |

---

# ğŸ“Œ **Future Enhancements**

* Add ML-based scoring (BERT, spaCy)
* Real-time monitoring dashboard
* API layer (FastAPI)
* Vector search with embeddings
* Rule auto-learning (machine learning)
* Docker deployment

---

## ğŸ‘¥ Contributors

### ğŸ‘¨â€ğŸ’¼ Project Lead  
- **Charan Teja Mangali** â€” Lead Developer, System Architect & Mentor

### ğŸ“ Student Contributors  
- **Student Name 1** â€”   
- **Student Name 2** â€”   
- **Student Name 3** â€”   
- **Student Name 4** â€”    

---

# â­ **Support the Project**

If you like this project, consider giving it a â­ on GitHub!

---

